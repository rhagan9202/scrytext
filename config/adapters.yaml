# Example adapter runtime configurations
# These show how to use adapters with actual data sources
# In production, pass these configs via API or code, not in files

examples:
  # JSON from file
  json_file:
    type: "json"
    source_id: "example-json-file"
    source_type: "file"
    path: "/data/sample.json"  # Runtime path
    use_cloud_processing: false
    
  # JSON from API
  json_api:
    type: "json"
    source_id: "example-api"
    source_type: "url"
    url: "https://api.example.com/data"  # Runtime URL
    use_cloud_processing: true
    headers:
      Authorization: "Bearer ${API_TOKEN}"

  # CSV from file
  csv_file:
    type: "csv"
    source_id: "example-csv-file"
    source_type: "file"
    path: "/data/sample.csv"  # Runtime path
    use_cloud_processing: false
    read_options:
      chunk_size: 65536  # 64KB chunks for steady memory usage
      max_bytes: 52428800  # 50MB safety limit

  # Excel from file
  excel_file:
    type: "excel"
    source_id: "example-excel-file"
    source_type: "file"
    path: "/data/sample.xlsx"  # Runtime path
    sheet_name: "Sheet1"  # Specify sheet at runtime
    use_cloud_processing: false
    read_options:
      chunk_size: 1048576  # 1MB chunks
      max_bytes: 104857600  # 100MB guardrail

  # REST API request
  rest_api:
    type: "rest"
    source_id: "example-rest-api"
    endpoint: "https://api.example.com/v1/resources"
    method: "GET"
    query_params:
      include: "summary"
    headers:
      Authorization: "Bearer ${API_TOKEN}"
    validation:
      expected_statuses: [200, 202]
    transformation:
      response_format: "json"

  # Web crawl using BeautifulSoup
  soup_crawl:
    type: "soup"
    source_id: "example-web-crawl"
    url: "https://example.com/articles"
    validation:
      required_selectors:
        - "article h1"
    transformation:
      selectors:
        headlines: "article h1"
        summaries: "article p.summary"
      include_raw: false