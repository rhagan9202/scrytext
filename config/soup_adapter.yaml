# BeautifulSoup Adapter Configuration
# Default settings for crawling HTML content and extracting structured data

# Processing mode
use_cloud_processing: false

# HTTP request settings
url: null
method: "GET"
timeout: 30  # seconds
headers: {}
query_params: {}
parser: "html.parser"  # Options: html.parser, lxml, html5lib
follow_redirects: false
max_content_length: 2097152  # 2 MB hard limit
allowed_hosts: []  # Optional host allowlist, supports wildcards like "*.example.com"
allowed_url_patterns: []  # Optional regex patterns evaluated against the full URL
allow_private_networks: false  # Explicit opt-in for private/loopback network targets

# Retry policy (disabled by default; enable for idempotent requests)
retry:
  enabled: false
  max_attempts: 3
  backoff_factor: 0.5
  max_backoff: 5.0
  jitter: 0.1
  status_forcelist: [429, 500, 502, 503, 504]
  retry_on_methods: ["GET", "HEAD"]

# Validation options
validation:
  expected_statuses: [200]
  min_content_length: 128
  max_content_length: 2097152  # 2 MB soft warning
  required_selectors: []  # CSS selectors that must be present in the HTML

# Transformation options
transformation:
  include_text: true
  text_separator: "\n"
  text_strip: true
  max_text_chars: null
  include_links: true
  include_metadata: true
  include_raw: false
  selectors: {}
